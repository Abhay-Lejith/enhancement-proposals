{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting: Model selection & evaluation\n",
    "\n",
    "Reference issue: [#622](https://github.com/alan-turing-institute/sktime/issues/622), [597](https://github.com/alan-turing-institute/sktime/issues/597)\n",
    "\n",
    "Contributors: @aiwalter, @mloning, @fkiraly, @pabworks, @ngupta23, @ViktorKaz\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We start by making a few conceptual points clarifying (i) the difference between model selection and model evaluation and (ii) different temporal cross-validation strategies. We then suggest possible design solutions. We conclude by highlighting a few technical challenges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "### Model selection vs model evaluation\n",
    "\n",
    "In model evaluation, we are interested in estimating model performance, that is, how the model is likely to perform in deployment. To estimate model performance, we typically use cross-validation. Our estimates are only reliable if are our assumptions hold in deployment. With time series data, for example, we cannot plausibly assume that our observations are i.i.d., and have to replace traditional estimation techniques such as cross-validation based on random sampling with techniques that take into account the temporal dependency structure of the data (e.g. temporal cross-validation techniques like sliding windows). \n",
    "\n",
    "In model selection, we are interested in selecting the best model from a predefined set of possible models, based on the best estimated model performance. So, model selection involves model evaluation, but having selected the best model, we still need to evaluate it in order to estimate its performance in deployment.\n",
    "\n",
    "Literature references:\n",
    "* [On the use of cross-validation for time series predictor evaluation](https://www.sciencedirect.com/science/article/pii/S0020025511006773?casa_token=3s0uDvJVsyUAAAAA:OSzMrqFwpjP-Rz3WKaftf8O7ZYdynoszegwgTsb-pYXAv7sRDtRbhihRr3VARAUTCyCmxjAxXqk), comparative empirical analysis of CV approaches for forecasting model evaluation\n",
    "* [On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation](https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf)\n",
    "\n",
    "### Different temporal cross-validation strategies\n",
    "\n",
    "There is a variety of different approaches to temporal cross-validation.\n",
    "\n",
    "Sampling: how is the data split into training and test windows\n",
    "* blocked cross-validation, random subsampling with some distance between training and test windows,\n",
    "* sliding windows, re-fitting the model for each training window (request [#621](https://github.com/alan-turing-institute/sktime/issues/621)),\n",
    "* sliding windows with an initial window, using the initial window for training and subsequent windows for updating,\n",
    "* expanding windows, refitting the model for each training window.\n",
    "\n",
    "It is important to document clearly which software specification implements which (statistical) strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design\n",
    "Since there is a clear difference between the concepts of model selection and evaluation, there should arguably also be a clear difference for the software API (following domain-driven design principles, more [here](https://arxiv.org/abs/2101.04938)). \n",
    "\n",
    "Potential design solutions:\n",
    "1. Keep `ForecastingGridSearchCV` and add model evaluation functionality\n",
    "2. Factor out model evaluation (e.g. `Evaluator`) and reuse it both inside model selection and for model evaluation functionality\n",
    "3. Keep only `ForecatingGridSearchCV` and use inspection on CV results for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Keep `ForecastingGridSearchCV` and add model evaluation functions \n",
    "see e.g. `cross_val_score` as in  [`pmdarima`](https://alkaline-ml.com/pmdarima/auto_examples/model_selection/example_cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(forecaster, y, fh, cv=None, strategy=\"refit\", scoring=None):\n",
    "    \"\"\"Evaluate forecaster using cross-validation\"\"\"\n",
    "    \n",
    "    # check cv, compatibility with fh\n",
    "    # check strategy, e.g. assert strategy in (\"refit\", \"update\"), compatibility with cv\n",
    "    # check scoring\n",
    "    \n",
    "    # pre-allocate score array\n",
    "    n_splits = cv.get_n_splits(y)\n",
    "    scores = np.empty(n_splits)\n",
    "    \n",
    "    for i, (train, test) in enumerate(cv.split(y)):\n",
    "        # split data\n",
    "        y_train = y.iloc[train]\n",
    "        y_test = y.iloc[test]\n",
    "        \n",
    "        # fit and predict\n",
    "        forecaster.fit(y_train, fh)\n",
    "        y_pred = forecaster.predict()\n",
    "        \n",
    "        # score\n",
    "        scores[i] = scoring(y_test, y_pred)\n",
    "    \n",
    "    # return scores, possibly aggregate\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Factor out model evaluation and reuse it both for model selection and model evaluation functionality\n",
    "For further modularizations, see current benchmarking module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using evaluate function from above\n",
    "class ForecastingGridSearchCV:\n",
    "    \n",
    "    def fit(self, y, fh=None, X=None):\n",
    "        # note that fh is no longer optional in fit here\n",
    "        \n",
    "        cv_results = np.empty(len(self.param_grid))\n",
    "        \n",
    "        for i, params in enumerate(self.param_grid):\n",
    "            forecaster = clone(self.forecaster)\n",
    "            forecaster.set_params(**params)\n",
    "            scores = evaluate(forecaster, y, fh, cv=self.cv, strategy=self.strategy, scoring=self.scoring)\n",
    "            cv_results[i] = np.mean(scores)\n",
    "            # note we need to keep track of more than just scores, including fitted models if we do \n",
    "            # not want to refit after model selection\n",
    "        \n",
    "        # select best params\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Keep only `ForecatingGridSearchCV` and use inspection on CV results for model evaluation\n",
    "basically possible now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical \n",
    "\n",
    "* Redundancy in model training in nested sliding/expanding window CV due to overlapping training windows, depending on step size, window length of both outer and inner CV, potential solution to avoid redunant training: some optimized class for nested CV that keeps track of windows and associated trained models. \n",
    "* Tuning with multi-step horizons required some data wrangling with pandas to present results for multi-windows and multi-step horizons (see [#633](https://github.com/alan-turing-institute/sktime/issues/633))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}